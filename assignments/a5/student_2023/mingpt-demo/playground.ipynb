{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5443, -0.6229,  0.7112, -1.2876],\n",
      "        [ 1.2251,  2.5207, -0.4421, -0.0687],\n",
      "        [-0.1830, -0.0266, -1.0737,  0.8168],\n",
      "        [ 1.1623,  1.4124,  2.2572,  0.2517]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a vocabulary and token IDs\n",
    "vocabulary = ['apple', 'banana', 'cherry', 'dog']\n",
    "token_ids = [0, 1, 2, 3]  # Example token IDs for 'apple', 'banana', 'cherry', 'dog'\n",
    "\n",
    "# Create an embedding layer\n",
    "embedding_dim = 4  # Dimensionality of token embeddings\n",
    "embedding_layer = nn.Embedding(len(vocabulary), embedding_dim)\n",
    "\n",
    "# Map token IDs to token embeddings\n",
    "token_embeddings = embedding_layer(torch.tensor(token_ids))\n",
    "\n",
    "print(token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "          0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "          0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "          0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  5.4030e-01,  5.3317e-01,  8.4601e-01,  3.1098e-01,\n",
      "          9.5042e-01,  1.7689e-01,  9.8423e-01,  9.9833e-02,  9.9500e-01,\n",
      "          5.6204e-02,  9.9842e-01,  3.1618e-02,  9.9950e-01,  1.7782e-02,\n",
      "          9.9984e-01,  9.9998e-03,  9.9995e-01,  5.6234e-03,  9.9998e-01,\n",
      "          3.1623e-03,  9.9999e-01,  1.7783e-03,  1.0000e+00,  1.0000e-03,\n",
      "          1.0000e+00,  5.6234e-04,  1.0000e+00,  3.1623e-04,  1.0000e+00,\n",
      "          1.7783e-04,  1.0000e+00],\n",
      "        [ 9.0930e-01, -4.1615e-01,  9.0213e-01,  4.3146e-01,  5.9113e-01,\n",
      "          8.0658e-01,  3.4821e-01,  9.3742e-01,  1.9867e-01,  9.8007e-01,\n",
      "          1.1223e-01,  9.9368e-01,  6.3203e-02,  9.9800e-01,  3.5558e-02,\n",
      "          9.9937e-01,  1.9999e-02,  9.9980e-01,  1.1247e-02,  9.9994e-01,\n",
      "          6.3245e-03,  9.9998e-01,  3.5566e-03,  9.9999e-01,  2.0000e-03,\n",
      "          1.0000e+00,  1.1247e-03,  1.0000e+00,  6.3246e-04,  1.0000e+00,\n",
      "          3.5566e-04,  1.0000e+00],\n",
      "        [ 1.4112e-01, -9.8999e-01,  9.9325e-01, -1.1597e-01,  8.1265e-01,\n",
      "          5.8275e-01,  5.0854e-01,  8.6104e-01,  2.9552e-01,  9.5534e-01,\n",
      "          1.6790e-01,  9.8580e-01,  9.4726e-02,  9.9550e-01,  5.3323e-02,\n",
      "          9.9858e-01,  2.9995e-02,  9.9955e-01,  1.6869e-02,  9.9986e-01,\n",
      "          9.4867e-03,  9.9995e-01,  5.3348e-03,  9.9999e-01,  3.0000e-03,\n",
      "          1.0000e+00,  1.6870e-03,  1.0000e+00,  9.4868e-04,  1.0000e+00,\n",
      "          5.3348e-04,  1.0000e+00],\n",
      "        [-7.5680e-01, -6.5364e-01,  7.7847e-01, -6.2768e-01,  9.5358e-01,\n",
      "          3.0114e-01,  6.5283e-01,  7.5751e-01,  3.8942e-01,  9.2106e-01,\n",
      "          2.2304e-01,  9.7481e-01,  1.2615e-01,  9.9201e-01,  7.1071e-02,\n",
      "          9.9747e-01,  3.9989e-02,  9.9920e-01,  2.2492e-02,  9.9975e-01,\n",
      "          1.2649e-02,  9.9992e-01,  7.1131e-03,  9.9997e-01,  4.0000e-03,\n",
      "          9.9999e-01,  2.2494e-03,  1.0000e+00,  1.2649e-03,  1.0000e+00,\n",
      "          7.1131e-04,  1.0000e+00],\n",
      "        [-9.5892e-01,  2.8366e-01,  3.2394e-01, -9.4608e-01,  9.9995e-01,\n",
      "         -1.0342e-02,  7.7653e-01,  6.3008e-01,  4.7943e-01,  8.7758e-01,\n",
      "          2.7748e-01,  9.6073e-01,  1.5746e-01,  9.8753e-01,  8.8797e-02,\n",
      "          9.9605e-01,  4.9979e-02,  9.9875e-01,  2.8113e-02,  9.9960e-01,\n",
      "          1.5811e-02,  9.9988e-01,  8.8913e-03,  9.9996e-01,  5.0000e-03,\n",
      "          9.9999e-01,  2.8117e-03,  1.0000e+00,  1.5811e-03,  1.0000e+00,\n",
      "          8.8914e-04,  1.0000e+00],\n",
      "        [-2.7942e-01,  9.6017e-01, -2.3037e-01, -9.7310e-01,  9.4715e-01,\n",
      "         -3.2080e-01,  8.7574e-01,  4.8278e-01,  5.6464e-01,  8.2534e-01,\n",
      "          3.3104e-01,  9.4362e-01,  1.8860e-01,  9.8205e-01,  1.0649e-01,\n",
      "          9.9431e-01,  5.9964e-02,  9.9820e-01,  3.3734e-02,  9.9943e-01,\n",
      "          1.8973e-02,  9.9982e-01,  1.0669e-02,  9.9994e-01,  6.0000e-03,\n",
      "          9.9998e-01,  3.3740e-03,  9.9999e-01,  1.8974e-03,  1.0000e+00,\n",
      "          1.0670e-03,  1.0000e+00],\n",
      "        [ 6.5699e-01,  7.5390e-01, -7.1372e-01, -7.0043e-01,  8.0042e-01,\n",
      "         -5.9944e-01,  9.4733e-01,  3.2026e-01,  6.4422e-01,  7.6484e-01,\n",
      "          3.8355e-01,  9.2352e-01,  2.1956e-01,  9.7560e-01,  1.2416e-01,\n",
      "          9.9226e-01,  6.9943e-02,  9.9755e-01,  3.9354e-02,  9.9923e-01,\n",
      "          2.2134e-02,  9.9976e-01,  1.2448e-02,  9.9992e-01,  6.9999e-03,\n",
      "          9.9998e-01,  3.9364e-03,  9.9999e-01,  2.2136e-03,  1.0000e+00,\n",
      "          1.2448e-03,  1.0000e+00],\n",
      "        [ 9.8936e-01, -1.4550e-01, -9.7726e-01, -2.1204e-01,  5.7432e-01,\n",
      "         -8.1863e-01,  9.8904e-01,  1.4763e-01,  7.1736e-01,  6.9671e-01,\n",
      "          4.3485e-01,  9.0050e-01,  2.5029e-01,  9.6817e-01,  1.4178e-01,\n",
      "          9.8990e-01,  7.9915e-02,  9.9680e-01,  4.4972e-02,  9.9899e-01,\n",
      "          2.5296e-02,  9.9968e-01,  1.4226e-02,  9.9990e-01,  7.9999e-03,\n",
      "          9.9997e-01,  4.4987e-03,  9.9999e-01,  2.5298e-03,  1.0000e+00,\n",
      "          1.4226e-03,  1.0000e+00],\n",
      "        [ 4.1212e-01, -9.1113e-01, -9.3982e-01,  3.4166e-01,  2.9126e-01,\n",
      "         -9.5664e-01,  9.9956e-01, -2.9651e-02,  7.8333e-01,  6.2161e-01,\n",
      "          4.8478e-01,  8.7464e-01,  2.8078e-01,  9.5977e-01,  1.5936e-01,\n",
      "          9.8722e-01,  8.9879e-02,  9.9595e-01,  5.0589e-02,  9.9872e-01,\n",
      "          2.8457e-02,  9.9960e-01,  1.6004e-02,  9.9987e-01,  8.9999e-03,\n",
      "          9.9996e-01,  5.0610e-03,  9.9999e-01,  2.8460e-03,  1.0000e+00,\n",
      "          1.6005e-03,  1.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def positional_encoding(seq_len, embedding_dim):\n",
    "    position = torch.arange(0, seq_len, dtype=torch.float32).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, embedding_dim, 2, dtype=torch.float32) * -(math.log(10000.0) / embedding_dim))\n",
    "    positional_encodings = torch.zeros(seq_len, embedding_dim)\n",
    "    positional_encodings[:, 0::2] = torch.sin(position * div_term)\n",
    "    positional_encodings[:, 1::2] = torch.cos(position * div_term)\n",
    "    return positional_encodings\n",
    "\n",
    "seq_len = 10  # Length of the sequence\n",
    "embedding_dim = 32  # Dimensionality of token embeddings\n",
    "positions = positional_encoding(seq_len, embedding_dim)\n",
    "\n",
    "print(positions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
